{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## <font color='Yellow'>Scrapping Indeed.com data</font> -->\n",
    "# Scrapping Indeed.com data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10'\n",
    "# URL='https://www.indeed.co.in/data-scientist-jobs-in-Bengaluru,-Karnataka'\n",
    "##conducting a request of the stated URL above:\n",
    "\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\", \"X-Requested-With\": \"XMLHttpRequest\"}\n",
    "page = requests.get(URL)#,verify=False)\n",
    "#specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#printing soup in a more structured tree format that makes for easier reading\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Sr. Data Scientist',\n",
       " 'Data Scientist, Data & Analytics (D&A)',\n",
       " 'Machine Learning Engineer',\n",
       " 'Associate Data Scientist',\n",
       " 'Data Scientist - FT New York, NY - #EBG0120-1',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist (BCMA)',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Machine learning intern and Data Scientist',\n",
       " 'Data Scientist (NLP Focused)',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Science/ML engineer with specialty in NLP',\n",
       " 'Sr. Data Analyst']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_job_title_from_result(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "#         print(div)\n",
    "      for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "#           print(a)\n",
    "          jobs.append(a['title'])\n",
    "    return(jobs)\n",
    "extract_job_title_from_result(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1\n",
      "Engel & Völkers Americas, Inc.\n",
      "15 1\n",
      "Strategic Financial Solutions\n",
      "13 1\n",
      "Ascensia Diabetes Care\n",
      "15 1\n",
      "Business & Decision\n",
      "16 1\n",
      "Triplebyte\n",
      "11 1\n",
      "Direct-to-Consumer and International\n",
      "12 1\n",
      "Empire Business Group LLC\n",
      "11 1\n",
      "PepsiCo\n",
      "11 1\n",
      "CITI\n",
      "14 1\n",
      "Digitalogy\n",
      "11 1\n",
      "Source Enterprises\n",
      "12 1\n",
      "Behold.ai\n",
      "12 1\n",
      "Fakespot\n",
      "11 1\n",
      "Cornell University\n",
      "11 1\n",
      "Kaplan\n",
      "16 1\n",
      "Transport Learning\n",
      "13 1\n",
      "Noom Inc.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Engel & Völkers Americas, Inc.',\n",
       " 'Strategic Financial Solutions',\n",
       " 'Ascensia Diabetes Care',\n",
       " 'Business & Decision',\n",
       " 'Triplebyte',\n",
       " 'Direct-to-Consumer and International',\n",
       " 'Empire Business Group LLC',\n",
       " 'PepsiCo',\n",
       " 'CITI',\n",
       " 'Digitalogy',\n",
       " 'Source Enterprises',\n",
       " 'Behold.ai',\n",
       " 'Fakespot',\n",
       " 'Cornell University',\n",
       " 'Kaplan',\n",
       " 'Transport Learning',\n",
       " 'Noom Inc.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_company_from_result(soup): \n",
    "    companies = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "#         print(len(div))\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "        print(len(div),len(company))\n",
    "#         print(company)\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "                print(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "            for span in sec_try:\n",
    "              companies.append(span.text.strip())\n",
    "    return(companies)\n",
    " \n",
    "extract_company_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['New York, NY',\n",
       " 'New York, NY',\n",
       " 'Valhalla, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'Ithaca, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "for div in soup.find_all(name='div', attrs={'class':'recJobLoc'}):\n",
    "#     print(len(div['data-rc-loc']))\n",
    "    a.append(div['data-rc-loc'])\n",
    "    \n",
    "    \n",
    "print(len(a))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York, NY\n",
      "New York, NY 10036\n",
      "New York, NY 10017 (Murray Hill area)\n",
      "New York, NY\n",
      "New York, NY\n",
      "New York, NY\n",
      "New York, NY\n",
      "New York, NY 10005 (Financial District area)\n",
      "Ithaca, NY 14853\n",
      "New York, NY 10019\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for div in soup.find_all(name='span', attrs={'class':'location'}):\n",
    "    print(div.text)\n",
    "    i=i+1\n",
    "    \n",
    "print(i)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_location_from_result(soup): \n",
    "  locations = []\n",
    "  spans = soup.findAll('span', attrs={'class':'location'})\n",
    "  for span in spans:\n",
    "    locations.append(span.text)\n",
    "  return(len(locations))\n",
    "extract_location_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engel & Völkers Americas, Inc.\n",
      "Strategic Financial Solutions\n",
      "Ascensia Diabetes Care\n",
      "Business & Decision\n",
      "Triplebyte\n",
      "*****\n",
      "<span class=\"salary no-wrap\">\n",
      "<span class=\"salaryText\">\n",
      "$125,000 - $190,000 a year</span>\n",
      "</span>\n",
      "*****\n",
      "data: $125,000 - $190,000 a year\n",
      "Direct-to-Consumer and International\n",
      "Empire Business Group LLC\n",
      "PepsiCo\n",
      "CITI\n",
      "Digitalogy\n",
      "*****\n",
      "<span class=\"salary no-wrap\">\n",
      "<span class=\"salaryText\">\n",
      "$50 - $80 an hour</span>\n",
      "</span>\n",
      "*****\n",
      "data: $50 - $80 an hour\n",
      "Source Enterprises\n",
      "Behold.ai\n",
      "Fakespot\n",
      "Cornell University\n",
      "Kaplan\n",
      "Transport Learning\n",
      "*****\n",
      "<span class=\"salary no-wrap\">\n",
      "<span class=\"salaryText\">\n",
      "$80,000 - $150,000 a year</span>\n",
      "</span>\n",
      "*****\n",
      "data: $80,000 - $150,000 a year\n",
      "Noom Inc.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['$125,000 - $190,000 a year',\n",
       " '$50 - $80 an hour',\n",
       " '$80,000 - $150,000 a year']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_salary_from_result(soup): \n",
    "  salaries = []\n",
    "  companies=[]\n",
    "  for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "#         print(div.text)\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "                print(b.text.strip())\n",
    "                \n",
    "           \n",
    "            for link in div.find_all(name=\"span\", attrs={\"class\":\"salary\"}):\n",
    "                print(\"\"\"*****\"\"\")\n",
    "                print(link)\n",
    "                print(\"\"\"*****\"\"\")\n",
    "                print(\"data:\", link.text.strip())\n",
    "\n",
    "                if link.text.strip() != '':\n",
    "                    salaries.append(link.text.strip())\n",
    "                else:\n",
    "                    try:\n",
    "                        div_two = div.find(name=\"span\", attrs={\"class\":\"salary\"})\n",
    "                        div_three = div_two.find(\"div\")\n",
    "                        salaries.append(div_three.text.strip())\n",
    "#                         salaries.append(\"Nothing_found\")\n",
    "\n",
    "                    except:\n",
    "                        salaries.append(\"Nothing_found\")\n",
    "        #         print(link)\n",
    "        else:\n",
    "            print(\"None\")\n",
    "              \n",
    "                \n",
    "                \n",
    "  return(salaries)\n",
    "extract_salary_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ Company :: Salaries }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Engel & Völkers Americas, Inc.': 'Nothing found',\n",
       " 'Strategic Financial Solutions': 'Nothing found',\n",
       " 'Ascensia Diabetes Care': 'Nothing found',\n",
       " 'Business & Decision': 'Nothing found',\n",
       " 'Triplebyte': '$125,000 - $190,000 a year',\n",
       " 'Direct-to-Consumer and International': 'Nothing found',\n",
       " 'Empire Business Group LLC': 'Nothing found',\n",
       " 'PepsiCo': 'Nothing found',\n",
       " 'CITI': 'Nothing found',\n",
       " 'Digitalogy': '$50 - $80 an hour',\n",
       " 'Source Enterprises': 'Nothing found',\n",
       " 'Behold.ai': 'Nothing found',\n",
       " 'Fakespot': 'Nothing found',\n",
       " 'Cornell University': 'Nothing found',\n",
       " 'Kaplan': 'Nothing found',\n",
       " 'Transport Learning': '$80,000 - $150,000 a year',\n",
       " 'Noom Inc.': 'Nothing found'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_salary(soup):\n",
    "    li=[]\n",
    "    salaries=[]\n",
    "    companies=[]\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "    #     print(div)\n",
    "        li.append(div)\n",
    "    #     print(type(div))\n",
    "#         print('####$$$$$$$$$####')\n",
    "#         print(len(div.find_all(name=\"span\", attrs={\"class\":\"salary\"})))\n",
    "        if len(div.find_all(name=\"span\", attrs={\"class\":\"salary\"})) ==0:\n",
    "            salaries.append(\"Nothing found\")\n",
    "        else:\n",
    "\n",
    "            for div_two in div.find_all(name=\"span\", attrs={\"class\":\"salary\"}):\n",
    "        #     div_three = div_two.find('div')\n",
    "\n",
    "        #     print(div_two.find_all(name=\"span\", attrs={\"class\":\"salaryText\"}))\n",
    "#                 print(div_two.text.strip())\n",
    "#                 print(type(div_two.text.strip()))\n",
    "                salaries.append(div_two.text.strip())\n",
    "    #             if type(div_two.text.strip()) == '':\n",
    "    #                 print(\"Null\")\n",
    "        #         print(div_two.text.strip())\n",
    "\n",
    "        #     asd=div_two.find_all(name=\"span\", attrs={\"class\":\"salaryText\"})\n",
    "        #     print(type(div_two))\n",
    "        #     if str(div_two) == []:\n",
    "        #         print('Nolee')\n",
    "#         print('####$$$$$$$$$####')\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "#         print(len(div),len(company))\n",
    "    #         print(company)\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "    #             print(b.text.strip())\n",
    "    #     link=div.find_all(name=\"span\", attrs={\"class\":\"salary\"})\n",
    "    #     print(link)\n",
    "\n",
    "\n",
    "    #     if True:\n",
    "    #         break\n",
    "    #     link=div.find(name=\"span\", attrs={\"class\":\"salary\"})\n",
    "    #     for span in div.find_all('span', recursive=False):\n",
    "    #             print('jkds')\n",
    "    #         print span.attrs['title']\n",
    "    #     print(link)\n",
    "    #         print(link)\n",
    "    #         company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "\n",
    "    # li[3]\n",
    "    print('{ Company :: Salaries }')\n",
    "    return (dict(zip(companies,salaries)))\n",
    "\n",
    "extract_salary(soup)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(salaries)\n",
    "# print(companies)\n",
    "# val=dict(zip(companies, salaries))\n",
    "# val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_from_result(soup): \n",
    "  summaries = []\n",
    "  spans = soup.findAll('div', attrs={'class': 'summary'})\n",
    "  for span in spans:\n",
    "    summaries.append(span.text.strip())\n",
    "  return(summaries)\n",
    "extract_summary_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Table for Job Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "max_results_per_city = 20\n",
    "city_set = [\"New+York\",\"Chicago\",\"San+Francisco\", \"Austin\", \"Seattle\", \"Los+Angeles\", \"Philadelphia\", \"Atlanta\", \"Dallas\", \"Pittsburgh\",\\\n",
    "            \"Portland\", \"Phoenix\",\\\n",
    "             \"Denver\", \"Houston\", \"Miami\", \"Washington+DC\", \"Boulder\"]\n",
    "\n",
    "\n",
    "columns = [\"city\", \"job_title\", \"company_name\", \"location\", \"summary\", \"salary\"]\n",
    "sample_df = pd.DataFrame(columns = columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping code:\n",
    "for city in city_set:\n",
    "    \n",
    "    \n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "#         print(start)\n",
    "        page = requests.get(\"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=\" + str(city) + \"&start=\" + str(start))\n",
    "        time.sleep(1)  #ensuring at least 1 second between page grabs\n",
    "        soup = BeautifulSoup(page.text, \"lxml\", from_encoding=\"utf-8\")\n",
    "        for div in soup.find_all(name='div', attrs={'class':'row'}): \n",
    "            \n",
    "        #specifying row num for index of job posting in dataframe\n",
    "            num = (len(sample_df) + 1) \n",
    "            #creating an empty list to hold the data for each posting\n",
    "            job_post = [] \n",
    "            #append city name\n",
    "            job_post.append(city) \n",
    "\n",
    "\n",
    "            ###---------------------###\n",
    "            #grabbing job title\n",
    "            for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "                job_post.append(a[\"title\"]) \n",
    "\n",
    "\n",
    "\n",
    "            ###---------------------###\n",
    "            #grabbing company name\n",
    "            company = div.find_all(name=\"span\", attrs={\"class\":\"company\"}) \n",
    "#             if len(company) == 0:\n",
    "#                 job_post.append(\"Company not availble\")\n",
    "                \n",
    "            if len(company) > 0: \n",
    "                for b in company:\n",
    "                    job_post.append(b.text.strip()) \n",
    "            else: \n",
    "                sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "                if len(sec_try) == 0:\n",
    "                    job_post.append(\"Company not available\")\n",
    "            \n",
    "                else:\n",
    "                    for span in sec_try:\n",
    "                        job_post.append(span.text.strip()) \n",
    "            \n",
    "\n",
    "            ###---------------------###\n",
    "            #grabbing location name\n",
    "            c = div.find_all(name='span', attrs={'class':'location'}) \n",
    "            if len(c) == 0:\n",
    "                job_post.append(\"Not Available loc\")\n",
    "            else:\n",
    "                for span in c:\n",
    "                    job_post.append(span.text) \n",
    "\n",
    "            ###---------------------###\n",
    "            #grabbing summary text\n",
    "            d = div.findAll('div', attrs={'class': 'summary'})\n",
    "            for span in d:\n",
    "                job_post.append(span.text.strip()) \n",
    "\n",
    "\n",
    "            ###---------------------###\n",
    "            #grabbing salary\n",
    "\n",
    "            try:\n",
    "                if len(div.find_all(name=\"span\", attrs={\"class\":\"salary\"})) ==0:\n",
    "                    job_post.append(\"Nothing found\")\n",
    "                    \n",
    "                else:\n",
    "                    for div_two in div.find_all(name=\"span\", attrs={\"class\":\"salary\"}):\n",
    "                        job_post.append(div_two.text.strip())\n",
    "            #             if type(div\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "#             print(job_post,len(job_post))\n",
    "\n",
    "            sample_df.loc[num] = job_post\n",
    "\n",
    "    #saving sample_df as a local csv file — define your own local path to save contents \n",
    "sample_df.to_csv(\"dataframe.csv\")#, encoding= \"utf-8\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Table Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Boulder',\n",
       " 'Data Scientist',\n",
       " 'Valen Analytics',\n",
       " 'Not Available loc',\n",
       " 'Join a high performing and rapidly growing team.We are currently looking to fill the following position in our Denver, CO office:',\n",
       " 'Nothing found']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>summary</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>New+York</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Engel &amp; Völkers Americas, Inc.</td>\n",
       "      <td>Not Available loc</td>\n",
       "      <td>Founded in Hamburg, Germany in 1977, Engel &amp; V...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>New+York</td>\n",
       "      <td>Data Scientist, Data &amp; Analytics (D&amp;A)</td>\n",
       "      <td>Business &amp; Decision</td>\n",
       "      <td>Not Available loc</td>\n",
       "      <td>\\*\\*\\*Insurance eligibility begins on the firs...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>New+York</td>\n",
       "      <td>Data Scientist (NLP Focused)</td>\n",
       "      <td>Fakespot</td>\n",
       "      <td>Not Available loc</td>\n",
       "      <td>Fakespot is looking for a data scientist to jo...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>New+York</td>\n",
       "      <td>Data Analyst/Data Scientist</td>\n",
       "      <td>Prospect 33</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Our client, a Tier 1 Investment bank is lookin...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>New+York</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Optima IT Recruitment</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>I was wondering whether you’d be open to looki...</td>\n",
       "      <td>$120,000 - $180,000 a year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Machine Learning &amp; Data Science Internship (Su...</td>\n",
       "      <td>Seagate Technology</td>\n",
       "      <td>Longmont, CO 80503</td>\n",
       "      <td>Seagate has an exciting internship at our Long...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Institutional Research - Senior Data Scientist...</td>\n",
       "      <td>University of Colorado Boulder</td>\n",
       "      <td>Boulder, CO 80309 (Colorado University area)</td>\n",
       "      <td>The Office of Data Analytics at the University...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Data Scientist- Relocation to Saudi Arabia</td>\n",
       "      <td>Saudi Aramco</td>\n",
       "      <td>Not Available loc</td>\n",
       "      <td>Saudi Aramco is seeking an Engineer for the de...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Senior Manager Data Science</td>\n",
       "      <td>CenturyLink</td>\n",
       "      <td>Not Available loc</td>\n",
       "      <td>The Field Operations &amp; Network Implementation ...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Valen Analytics</td>\n",
       "      <td>Not Available loc</td>\n",
       "      <td>Join a high performing and rapidly growing tea...</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>527 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city                                          job_title  \\\n",
       "1    New+York                                     Data Scientist   \n",
       "2    New+York             Data Scientist, Data & Analytics (D&A)   \n",
       "3    New+York                       Data Scientist (NLP Focused)   \n",
       "4    New+York                        Data Analyst/Data Scientist   \n",
       "5    New+York                                     Data Scientist   \n",
       "..        ...                                                ...   \n",
       "523   Boulder  Machine Learning & Data Science Internship (Su...   \n",
       "524   Boulder  Institutional Research - Senior Data Scientist...   \n",
       "525   Boulder         Data Scientist- Relocation to Saudi Arabia   \n",
       "526   Boulder                        Senior Manager Data Science   \n",
       "527   Boulder                                     Data Scientist   \n",
       "\n",
       "                       company_name  \\\n",
       "1    Engel & Völkers Americas, Inc.   \n",
       "2               Business & Decision   \n",
       "3                          Fakespot   \n",
       "4                       Prospect 33   \n",
       "5             Optima IT Recruitment   \n",
       "..                              ...   \n",
       "523              Seagate Technology   \n",
       "524  University of Colorado Boulder   \n",
       "525                    Saudi Aramco   \n",
       "526                     CenturyLink   \n",
       "527                 Valen Analytics   \n",
       "\n",
       "                                         location  \\\n",
       "1                               Not Available loc   \n",
       "2                               Not Available loc   \n",
       "3                               Not Available loc   \n",
       "4                                    New York, NY   \n",
       "5                                    New York, NY   \n",
       "..                                            ...   \n",
       "523                            Longmont, CO 80503   \n",
       "524  Boulder, CO 80309 (Colorado University area)   \n",
       "525                             Not Available loc   \n",
       "526                             Not Available loc   \n",
       "527                             Not Available loc   \n",
       "\n",
       "                                               summary  \\\n",
       "1    Founded in Hamburg, Germany in 1977, Engel & V...   \n",
       "2    \\*\\*\\*Insurance eligibility begins on the firs...   \n",
       "3    Fakespot is looking for a data scientist to jo...   \n",
       "4    Our client, a Tier 1 Investment bank is lookin...   \n",
       "5    I was wondering whether you’d be open to looki...   \n",
       "..                                                 ...   \n",
       "523  Seagate has an exciting internship at our Long...   \n",
       "524  The Office of Data Analytics at the University...   \n",
       "525  Saudi Aramco is seeking an Engineer for the de...   \n",
       "526  The Field Operations & Network Implementation ...   \n",
       "527  Join a high performing and rapidly growing tea...   \n",
       "\n",
       "                         salary  \n",
       "1                 Nothing found  \n",
       "2                 Nothing found  \n",
       "3                 Nothing found  \n",
       "4                 Nothing found  \n",
       "5    $120,000 - $180,000 a year  \n",
       "..                          ...  \n",
       "523               Nothing found  \n",
       "524               Nothing found  \n",
       "525               Nothing found  \n",
       "526               Nothing found  \n",
       "527               Nothing found  \n",
       "\n",
       "[527 rows x 6 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "# URL = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "\n",
    "# data=requests.get(URL,verify=False)\n",
    "# soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "# results = soup.find_all('div', attrs={'data-tn-component': 'organicJob'})\n",
    "# print(soup)\n",
    "\n",
    "# for x in results:\n",
    "#     company = x.find('span', attrs={\"itemprop\":\"name\"})\n",
    "#     print ('company:', company.text.strip())\n",
    "\n",
    "#     job = x.find('a', attrs={'data-tn-element': \"jobTitle\"})\n",
    "#     print ('job:', job.text.strip())\n",
    "\n",
    "#     salary = x.find('nobr')\n",
    "#     if salary:\n",
    "#         print('salary:', salary.text.strip())\n",
    "\n",
    "#     print('----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
